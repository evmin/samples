{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach Overview\n",
    "\n",
    "Two chunking methods combined:\n",
    "\n",
    " 1. Splitting by the markdown headers. Expecting the input to be pre-processed by document intelligence (or other tools).\n",
    " 2. If chunks exceed MAX_CHUNK_TOKEN_SIZE, split further by using semantic chunker.\n",
    " 3. If the resulting semantic chunk is still larger than MAX_CHUNK_TOKEN_SIZE, use Semantic Chunking on this chunk alone. \n",
    " No recursion, just put it back into queue .\n",
    "\n",
    "\n",
    "Using Langchain splitters. Also included Llama Index Semantic Chunker as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken langchain_experimental langchain_openai langchain-text-splitters python-dotenv llama-index llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "MAX_CHUNK_TOKEN_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ SEMANTIC SPLITTER ]\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment = os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "    azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    openai_api_key = os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    openai_api_version = \"2024-02-01\"\n",
    ")\n",
    "\n",
    "# Uses cosine similarity to determine the semantic similarity between two chunks. \n",
    "# If the similarity is below the threshold, the sentences will be merged into the same chunk.\n",
    "# Otherwise - starting a new chunk\n",
    "semantic_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\", breakpoint_threshold_amount=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ MARKDOWN SPLITTER ] \n",
    "# The second value (like \"header_1\") becomes a key in the metadata output from the splitter\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"header_1\"),\n",
    "    (\"##\", \"header_2\"),\n",
    "    (\"###\", \"header_3\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_len(input: str) -> int:\n",
    "    \"\"\"Returns number of tokens for the input. Only for models > gpt-3.5 supported as we use 'cl100k_base' encoding.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\") \n",
    "    return len(encoding.encode(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_semantic_split(pages):\n",
    "    queue = deque(s for s in pages)  # Initialize queue \n",
    "    result = []\n",
    "\n",
    "    while queue:\n",
    "        current_page = queue.popleft()\n",
    "        # If the chunk is too large, semantically split it into smaller chunks and add them back to the queue, to check if they fit the token limit\n",
    "        if token_len(current_page) > MAX_CHUNK_TOKEN_SIZE:\n",
    "            semantic_chunks = semantic_splitter.split_text(current_page)\n",
    "            for s_chunk in reversed(semantic_chunks):\n",
    "                queue.appendleft(s_chunk) \n",
    "        else:\n",
    "            result.append(current_page)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ TEST ] \n",
    "# Reading source document\n",
    "# Normally would be expecting here a markdown document, but for the sake of the example we are using a plain text\n",
    "with open(\"./sample_data/Tesla_Model_S.txt\") as f:\n",
    "    input_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown split \n",
    "md_header_splits = markdown_splitter.split_text(input_text)\n",
    "plain_content = [ value.page_content for value in md_header_splits] # Extracting the text content from markdown split result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic split\n",
    "semantic_list = process_semantic_split(plain_content)\n",
    "\n",
    "print(\"Number of semantic chunks:\"+str(len(semantic_list)))\n",
    "for split in semantic_list:\n",
    "    print(token_len(split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results\n",
    "with open('slit_results.txt', 'w') as file:\n",
    "    for item in semantic_list:\n",
    "        file.write(f\"------------\\n\")\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL EXPERIMENTS \n",
    "Experimenting with LLAMA - INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.readers.file import FlatReader\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core import SimpleDirectoryReader  \n",
    "from llama_index.core import Settings  \n",
    "from llama_index.core.node_parser import (  \n",
    "    SemanticSplitterNodeParser,  \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "    api_key= os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_version= \"2024-02-01\"\n",
    ")\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally would be expecting here a markdown document, but for the sake of the example we are using a plain text\n",
    "md_docs = FlatReader().load_data(Path(\"./sample_data/Tesla_Model_S.txt\"))\n",
    "parser = MarkdownNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(md_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_nodes = splitter.get_nodes_from_documents(nodes)  \n",
    "\n",
    "print(len(split_nodes))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_nodes[6].get_content())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
